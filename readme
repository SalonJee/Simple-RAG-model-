RAG( Retrieval Augmented Generation) systems optimize the output of a LLM , so it references an external authorative knowledge base outside of its training data sources before generating a response. It extends the already powerful capability of a LLM to specific domain , without the need of retraining the model. 

A RAG system consists of two key components : 

A retreival Model  and a Language Model

We use cosine simlarity to retrieve the relevant info(chunks), with the help of vector embedding .We used

For embeding model : compendiumlabs model of Ollama
for language model : bartowski model of Ollama



#1 clone the repo

#2 install the required Python libraries : 
pip instlal ollama

#3 pull/download the necessary models from ollama( in terminal)

ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf
ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF

#4 run the script " python3 model.py"

#5 do as per the instruction to get you query answered
